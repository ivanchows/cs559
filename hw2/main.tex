\documentclass{exam}

\usepackage{amsmath}

\usepackage{amssymb}

\usepackage{graphicx}

\usepackage{cite}
\usepackage{color} 
\usepackage{setspace}
\usepackage{hyperref}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{tcolorbox}
\usepackage{hyperref}
\newcommand{\xx}{{\bf{x}}}
\newcommand{\yy}{{\bf{y}}}
\newcommand{\ww}{{\bf{w}}}
\newcommand{\uu}{{\bf{u}}}

\pagestyle{headandfoot}
\runningheadrule
\firstpageheader{CS559: Machine Learning}{Name: Ivan Chow}{\textcolor{red}{Due: Oct. 3, 2024}}

\title{Assignment 2 MLE and MAP (total 20 pts)}
\date{}
\begin{document}
\maketitle
\thispagestyle{headandfoot}

\begin{center}
  {\fbox{\parbox{5.5in}{\centering
Homework assignments will be done individually: each student must hand in their own answers. Use of partial or entire solutions obtained from others or online is strictly prohibited. Electronic submission on Canvas is mandatory.}}}
\end{center}
\vspace{.5cm}

\underline{\bf Please follow the below instructions when you submit the assignment.}
\begin{itemize}
\item Your submission should consist of a zip file named A2\_FirstName\_LastName.zip which contains: 
\begin{itemize}
\item  a pdf file to show the derivation steps of for questions 1 to 4.
\end{itemize}
\end{itemize}

\begin{questions}

\question{\bf  Maximum Likelihood estimator} (5 points) Assuming data points are independent and identically distributed (i.i.d.), the probability of the data set given parameters: $\mu$ and $\sigma^2$ (the likelihood function):
\begin{align}
\nonumber P(\mathbf{x}|\mu,\sigma^2) = \prod_{n=1}^N\mathcal{N}(x_n|\mu,\sigma^2)
\end{align}

Please calculate the solution for $\mu$ and $\sigma^2$ using Maximum Likelihood (ML) estimator.


\vspace{2mm}\hrule\vspace{2mm} 

\textbf{Solution.} \\[0.5mm]  
%%%%%%%%%%%%%%%%%%%% YOUR SOLUTION COMES HERE -- REPLACE WITH YOUR OWN TEXT %%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%  END OF YOUR SOLUTION %%%%%%%%%%%%%%%%%%%%%%%%

\vspace{2mm}\hrule\vspace{2mm}


\newpage
\question{\bf Maximum Likelihood} (5 points) We assume there is a true function $f(\xx)$ and the target value is given by $y=f(x)+\epsilon$ where $\epsilon$ is a Gaussian distribution with mean $0$ and variance $\sigma^2$.
Thus,
$$p(y|x,w,\beta) =\mathcal{N}(y| f(x), \beta^{-1})$$

where $\beta^{-1} = \sigma^2$.

Assuming the data points are drawn independently from the distribution, we obtain the likelihood function:
$$p(\mathbf{y}|\xx,w,\beta) = \prod_{n=1}^N \mathcal{N}(y_n|f(x),\beta^{-1})$$

Please show that maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function.

\vspace{2mm}\hrule\vspace{2mm} 

\textbf{Solution.} \\[0.5mm]  
%%%%%%%%%%%%%%%%%%%% YOUR SOLUTION COMES HERE -- REPLACE WITH YOUR OWN TEXT %%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%  END OF YOUR SOLUTION %%%%%%%%%%%%%%%%%%%%%%%%

\vspace{2mm}\hrule\vspace{2mm}

\newpage
\question{\bf  MAP estimator} (5 points) Given input values $\xx= (x_1,...,x_N)^T$ and their corresponding target values $\yy= (y_1,...,y_N)^T$, we estimate the target by using function $f(x,\ww)$ which is a polynomial curve. Assuming the target variables are drawn from Gaussian distribution:

$$p(y|x, \ww,\beta) = \mathcal{N} (y | f(x,\ww), \beta^{-1})$$

and  a prior Gaussian distribution for $\ww$:

$$p(\ww|\alpha) = (\frac{\alpha}{2\pi})^{(M+1)/2} \exp(-\frac{\alpha}{2} \ww^T\ww)$$

Please prove that maximum posterior (MAP) is equivalent to minimizing the regularized sum-of-squares error function. Note that the posterior distribution of $\ww$ is $p(\ww|\xx,\yy,\alpha,\beta)$. \textbf{Hint: use Bayes' theorem.}

\vspace{2mm}\hrule\vspace{2mm} 

\textbf{Solution.} \\[0.5mm]  
%%%%%%%%%%%%%%%%%%%% YOUR SOLUTION COMES HERE -- REPLACE WITH YOUR OWN TEXT %%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%  END OF YOUR SOLUTION %%%%%%%%%%%%%%%%%%%%%%%%

\vspace{2mm}\hrule\vspace{2mm}


\newpage
\question{\bf  Linear model} (5 points) Consider a linear model of the form:
$$f(\xx,\ww) = w_0 + \sum_{i=1}^D w_i x_i$$
together with a sum-of-squares error/loss function of the form:
$$L_D(\ww) = \frac{1}{2} \sum_{n=1}^N \{f(\xx_n,\ww) - y_n\}^2$$
Now suppose that Gaussian noise $\epsilon_i$ with zero mean and variance $\sigma^2$ is added independently to each of the input variables $x_i$. By making use of $\mathbb{E}[\epsilon_i]=0$ and $\mathbb{E}[\epsilon_i\epsilon_j]=\delta_{ij} \sigma^2$ where $\delta_{ii}=1$, show that minimizing $L_D$ averaged over the noise distribution is equivalent to minimizing the sum-of-squares error
for noise-free input variables with the addition of a weight-decay regularization term, in which the bias parameter $w_0$ is omitted from the regularize.

\vspace{2mm}\hrule\vspace{2mm} 

\textbf{Solution.} \\[0.5mm]  
%%%%%%%%%%%%%%%%%%%% YOUR SOLUTION COMES HERE -- REPLACE WITH YOUR OWN TEXT %%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%  END OF YOUR SOLUTION %%%%%%%%%%%%%%%%%%%%%%%%

\vspace{2mm}\hrule\vspace{2mm}


 
\end{questions}




\end{document}